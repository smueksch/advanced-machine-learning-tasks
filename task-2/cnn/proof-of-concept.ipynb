{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "sys.path.append(os.path.join(os.pardir, os.pardir))\n",
    "from amlutils.task2.loading import load_train_set\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/80 [11:15<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x17832</th>\n",
       "      <th>x17833</th>\n",
       "      <th>x17834</th>\n",
       "      <th>x17835</th>\n",
       "      <th>x17836</th>\n",
       "      <th>x17837</th>\n",
       "      <th>x17838</th>\n",
       "      <th>x17839</th>\n",
       "      <th>x17840</th>\n",
       "      <th>x17841</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-64</td>\n",
       "      <td>-66</td>\n",
       "      <td>-69</td>\n",
       "      <td>-72</td>\n",
       "      <td>-75</td>\n",
       "      <td>-77</td>\n",
       "      <td>-80</td>\n",
       "      <td>-86</td>\n",
       "      <td>-89</td>\n",
       "      <td>-83</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>505</td>\n",
       "      <td>500</td>\n",
       "      <td>496</td>\n",
       "      <td>492</td>\n",
       "      <td>487</td>\n",
       "      <td>480</td>\n",
       "      <td>475</td>\n",
       "      <td>476</td>\n",
       "      <td>483</td>\n",
       "      <td>495</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-21</td>\n",
       "      <td>-16</td>\n",
       "      <td>-12</td>\n",
       "      <td>-7</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-211</td>\n",
       "      <td>-457</td>\n",
       "      <td>-635</td>\n",
       "      <td>-710</td>\n",
       "      <td>-715</td>\n",
       "      <td>-663</td>\n",
       "      <td>-573</td>\n",
       "      <td>-481</td>\n",
       "      <td>-401</td>\n",
       "      <td>-337</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5112</th>\n",
       "      <td>-285</td>\n",
       "      <td>-303</td>\n",
       "      <td>-334</td>\n",
       "      <td>-376</td>\n",
       "      <td>-413</td>\n",
       "      <td>-432</td>\n",
       "      <td>-443</td>\n",
       "      <td>-451</td>\n",
       "      <td>-460</td>\n",
       "      <td>-468</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5113</th>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>44</td>\n",
       "      <td>42</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>-207</td>\n",
       "      <td>-225</td>\n",
       "      <td>-242</td>\n",
       "      <td>-258</td>\n",
       "      <td>-266</td>\n",
       "      <td>-271</td>\n",
       "      <td>-275</td>\n",
       "      <td>-279</td>\n",
       "      <td>-281</td>\n",
       "      <td>-284</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5115</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5116</th>\n",
       "      <td>130</td>\n",
       "      <td>11</td>\n",
       "      <td>-154</td>\n",
       "      <td>-331</td>\n",
       "      <td>-472</td>\n",
       "      <td>-513</td>\n",
       "      <td>-506</td>\n",
       "      <td>-426</td>\n",
       "      <td>-324</td>\n",
       "      <td>-225</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5117 rows × 17842 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       x0   x1   x2   x3   x4   x5   x6   x7   x8   x9  ...  x17832  x17833  \\\n",
       "id                                                      ...                   \n",
       "0     -64  -66  -69  -72  -75  -77  -80  -86  -89  -83  ...     NaN     NaN   \n",
       "1     505  500  496  492  487  480  475  476  483  495  ...     NaN     NaN   \n",
       "2     -21  -16  -12   -7   -3    0    1    2    4    5  ...     NaN     NaN   \n",
       "3    -211 -457 -635 -710 -715 -663 -573 -481 -401 -337  ...     NaN     NaN   \n",
       "4      36   32   29   25   22   19   17   15   12   10  ...     NaN     NaN   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...     ...     ...   \n",
       "5112 -285 -303 -334 -376 -413 -432 -443 -451 -460 -468  ...     NaN     NaN   \n",
       "5113   50   51   50   48   46   44   42   39   36   33  ...     NaN     NaN   \n",
       "5114 -207 -225 -242 -258 -266 -271 -275 -279 -281 -284  ...     NaN     NaN   \n",
       "5115   13   16   18   21   23   24   25   27   29   32  ...     NaN     NaN   \n",
       "5116  130   11 -154 -331 -472 -513 -506 -426 -324 -225  ...     NaN     NaN   \n",
       "\n",
       "      x17834  x17835  x17836  x17837  x17838  x17839  x17840  x17841  \n",
       "id                                                                    \n",
       "0        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "1        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "2        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "3        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "4        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "5112     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "5113     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "5114     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "5115     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "5116     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "\n",
       "[5117 rows x 17842 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_orig, y_train_orig = load_train_set(os.path.join(os.pardir, 'data'))\n",
    "display(X_train_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_orig.fillna(0.0),\n",
    "    y_train_orig,\n",
    "    test_size=0.2,\n",
    "    shuffle=False,\n",
    "    stratify=None\n",
    ")\n",
    "\n",
    "def convert_to_tensor(X):\n",
    "    X_tensor = torch.tensor(X.values, dtype=torch.float)\n",
    "    # Unsqueeze X tensor to have another dimension representing the channel, this\n",
    "    # is needed for convolutions.\n",
    "    X_tensor = torch.unsqueeze(X_tensor, 1)\n",
    "    return X_tensor\n",
    "\n",
    "def build_data_loader(X, y):\n",
    "    X_tensor = convert_to_tensor(X)\n",
    "    y_tensor = torch.tensor(y.values)\n",
    "\n",
    "    train_tensor = TensorDataset(X_tensor, y_tensor)\n",
    "    return DataLoader(dataset=train_tensor, batch_size=64, shuffle=True)\n",
    "\n",
    "train_loader = build_data_loader(X_train, y_train)\n",
    "valid_loader = build_data_loader(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4093"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2436, 1168,  360,  129])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.59516247, 0.28536526, 0.08795505, 0.03151722])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.68021346,  3.50428082, 11.36944444, 31.72868217])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.03479955, 0.07257851, 0.23547695, 0.65714498])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_samples = y_train.shape[0]\n",
    "num_samples_per_class = y_train.value_counts().values\n",
    "\n",
    "class_ratios = num_samples_per_class / num_samples\n",
    "class_inverse_ratios = 1 / class_ratios\n",
    "\n",
    "normalized_class_inverse_ratios = class_inverse_ratios / class_inverse_ratios.sum()\n",
    "\n",
    "display(num_samples)\n",
    "display(num_samples_per_class)\n",
    "display(class_ratios)\n",
    "display(class_inverse_ratios)\n",
    "display(normalized_class_inverse_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ECGCNN                                   --                        --\n",
       "├─Sequential: 1-1                        [64, 4]                   --\n",
       "│    └─Conv1d: 2-1                       [64, 3, 8905]             39\n",
       "│    └─BatchNorm1d: 2-2                  [64, 3, 8905]             6\n",
       "│    └─ReLU: 2-3                         [64, 3, 8905]             --\n",
       "│    └─Dropout: 2-4                      [64, 3, 8905]             --\n",
       "│    └─MaxPool1d: 2-5                    [64, 3, 4452]             --\n",
       "│    └─Conv1d: 2-6                       [64, 6, 2221]             114\n",
       "│    └─BatchNorm1d: 2-7                  [64, 6, 2221]             12\n",
       "│    └─ReLU: 2-8                         [64, 6, 2221]             --\n",
       "│    └─Dropout: 2-9                      [64, 6, 2221]             --\n",
       "│    └─MaxPool1d: 2-10                   [64, 6, 1110]             --\n",
       "│    └─Conv1d: 2-11                      [64, 3, 554]              57\n",
       "│    └─BatchNorm1d: 2-12                 [64, 3, 554]              6\n",
       "│    └─ReLU: 2-13                        [64, 3, 554]              --\n",
       "│    └─Dropout: 2-14                     [64, 3, 554]              --\n",
       "│    └─MaxPool1d: 2-15                   [64, 3, 276]              --\n",
       "│    └─Flatten: 2-16                     [64, 828]                 --\n",
       "│    └─Linear: 2-17                      [64, 4]                   3,316\n",
       "==========================================================================================\n",
       "Total params: 3,550\n",
       "Trainable params: 3,550\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 40.67\n",
       "==========================================================================================\n",
       "Input size (MB): 4.57\n",
       "Forward/backward pass size (MB): 42.71\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 47.29\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Architecture based on: https://pythonwife.com/convolutional-autoencoders-opencv/\n",
    "\n",
    "class ECGCNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=3, kernel_size=12, stride=2, dilation=3),\n",
    "            nn.BatchNorm1d(num_features=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2),\n",
    "            nn.Conv1d(in_channels=3, out_channels=6, kernel_size=6, stride=2, dilation=2),\n",
    "            nn.BatchNorm1d(num_features=6),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2),\n",
    "            nn.Conv1d(in_channels=6, out_channels=3, kernel_size=3, stride=2, dilation=1),\n",
    "            nn.BatchNorm1d(num_features=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(out_features=4)\n",
    "        )\n",
    "\n",
    "        # Dummy forward pass to initialize Lazy* layers.\n",
    "        self.layers(torch.ones(10, 1, 17842))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return F.softmax(self.layers(X), dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        # For cross-entropy loss, require that y.shape == (batch_size), but\n",
    "        # y has shape (batch_size, 1) so squeeze out unnecessary dimension.\n",
    "        y = torch.squeeze(y)\n",
    "\n",
    "        y_pred = self.layers(X)\n",
    "        loss = F.cross_entropy(\n",
    "            y_pred,\n",
    "            y,\n",
    "            weight=torch.FloatTensor(normalized_class_inverse_ratios)\n",
    "        )\n",
    "\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        # For cross-entropy loss, require that y.shape == (batch_size), but\n",
    "        # y has shape (batch_size, 1) so squeeze out unnecessary dimension.\n",
    "        y = torch.squeeze(y)\n",
    "\n",
    "        y_pred = self.layers(X)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "\n",
    "        self.log('valid_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "torchinfo.summary(ECGCNN(), input_size=(64, 1, 17842))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 3.6 K \n",
      "--------------------------------------\n",
      "3.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 K     Total params\n",
      "0.014     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:453: UserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/80 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 80/80 [00:14<00:00,  5.47it/s, loss=0.809, v_num=14]\n"
     ]
    }
   ],
   "source": [
    "ecg_cnn = ECGCNN()\n",
    "\n",
    "trainer = pl.Trainer(callbacks=[EarlyStopping(monitor='valid_loss')])\n",
    "trainer.fit(ecg_cnn, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint('ecg-cnn-weighted-crossentropy.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECG-CNN validation F1 micro score: 0.314453125\n"
     ]
    }
   ],
   "source": [
    "valid_score = f1_score(\n",
    "    y_valid,\n",
    "    ecg_cnn(convert_to_tensor(X_valid)).detach().numpy().argmax(axis=1),\n",
    "    average='micro'\n",
    ")\n",
    "print(f'ECG-CNN validation F1 micro score: {valid_score}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
